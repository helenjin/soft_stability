{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc12fcef-5f5c-480f-a905-cbc19ac93069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import ViTForImageClassification\n",
    "import numpy as np\n",
    "\n",
    "import sys; sys.path.append(\"../src/\")\n",
    "from models import SmoothMaskedImageClassifier\n",
    "from data_utils import load_images_from_directory\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2f4e0ad-47e4-443b-8984-9bad94acd2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_stuff = True\n",
    "overwrite_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a599ae9-7c66-4055-9edd-5a6fd4f34eba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "248fa293-0a88-4c52-a9fc-df2c52aba7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "images = load_images_from_directory(\"/home/antonxue/foo/imagenet-sample-images/\").to(device)\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23c54c6e-548d-46d8-aa37-aafdd202dcb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.25, 0.2, 0.1]\n"
     ]
    }
   ],
   "source": [
    "lambdas = [1.0, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.25, 0.2, 0.1]\n",
    "print(lambdas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36b7da2e-d717-4ccd-b618-98c355e67f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72576934-fc59-4d88-8d39-199179fc5ed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97044a45fd024858b9af64b53b443fd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a76b76e0be94343903743209f081a26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m class_id, image \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pbar):\n\u001b[1;32m     13\u001b[0m     out \u001b[38;5;241m=\u001b[39m smooth_vit(image[\u001b[38;5;28;01mNone\u001b[39;00m,\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m])\n\u001b[0;32m---> 14\u001b[0m     hits \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mclass_id\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     num_dones \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     16\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlambda \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlambda_\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, acc \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(hits\u001b[38;5;241m/\u001b[39mnum_dones)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if run_stuff:\n",
    "    for lambda_ in lambdas:\n",
    "        smooth_vit = SmoothMaskedImageClassifier(\n",
    "            ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224\"),\n",
    "            num_samples = 64,\n",
    "            lambda_ = lambda_\n",
    "        ).eval().to(device)\n",
    "    \n",
    "        hits, num_dones = 0, 0\n",
    "        pbar = tqdm(images)\n",
    "        with torch.no_grad():\n",
    "            for class_id, image in enumerate(pbar):\n",
    "                out = smooth_vit(image[None,...])\n",
    "                hits += (out.argmax(dim=-1) == class_id).item()\n",
    "                num_dones += 1\n",
    "                pbar.set_description(f\"lambda {lambda_:.3f}, acc {(hits/num_dones):.3f}\")\n",
    "        accs.append(hits/num_dones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4955e9-3533-4f8f-b405-88820c9eccfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6b6864-a727-4cad-8b7a-52bb00422864",
   "metadata": {},
   "outputs": [],
   "source": [
    "if overwrite_cache and len(accs) > 0:\n",
    "    torch.save({\"lambdas\":lambdas, \"accs\":accs}, \"_cache/vit_accuracy_vs_lambda.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46d805b-7f42-4a83-9cf1-1d61f54fb2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accs = torch.load(\"_cache/roberta_accuracy_vs_lambda.pt\")[\"accs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc28b8a-07d2-447f-ae69-aa2ba3397e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = 12\n",
    "plt.clf()\n",
    "fig, ax = plt.subplots(figsize=(4,2))\n",
    "ax.plot(lambdas, accs)\n",
    "plt.gca().invert_xaxis()\n",
    "\n",
    "ax.set_xticks([1.0, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1])\n",
    "ax.set_ylabel(\"Accuracy\", fontsize=fs)\n",
    "ax.set_xlabel(\"Smoothing Parameter $\\lambda$\", fontsize=fs)\n",
    "\n",
    "ax.tick_params(axis=\"both\", labelsize=fs-2)\n",
    "ax.axvline(x=1.0, color=\"green\", linestyle=\"--\", linewidth=2, label=f\"{accs[0]:.3f}\")\n",
    "ax.axvline(x=0.5, color=\"orange\", linestyle=\"--\", linewidth=2, label=f\"{accs[5]:.3f}\")\n",
    "ax.axvline(x=0.25, color=\"red\", linestyle=\"--\", linewidth=2, label=f\"{accs[7]:.3f}\")\n",
    "\n",
    "ax.legend(title=\"ViT\", loc=\"lower left\", title_fontsize=fs, fontsize=fs)\n",
    "plt.savefig(\"../figures/vit_accuracy_vs_lambda.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a734365-814d-4d8b-a77c-3908c18bd504",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
